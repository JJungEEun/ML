# 스터디 1주차(chap2)

# 2. 간단한 분류 알고리즘 훈련

## 2.1 인공 뉴런: 초기 머신 러닝의 간단한 역사

- 퍼셉트론
- 2.1.1 인공 뉴런의 수학적 정의
    
    인공 뉴런(artifical neuron) 아이디어를 두 개의 클래스가 있는 이진 분류 작업으로 볼 수 있다. 두 클래스는 1(양성 클래스)과 -1(음성 클래스)로 나뉜다. 그 다음 입력 값 x와 이에 상응하는 가중치 벡터 w의 선형 조합으로 결정 함수(ϕ(z))를 정의한다. 최종입력(net input)인 z는 z=w1x1+ w2x2+...+wmxm이다. 특정 샘플 x(i)의 최종 입력이 사전에 정의된 임계 값 θ보다 크면 클래스 1로, 그렇지 않으면 클래스 -1로 예측한다. 
    
    임계 값 θ를 식의 왼쪽으로 옮겨 w0 - -θ고 xo=1인 0번째 가중치를 정의한다.
     z = w0x0 + w1x1 + ... + wmxm = wTx
    
    결정 함수는 다음과 같다.
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled.png)
    
    머신러닝 분야에서는 음수 임계값 또는 가중치 w0 - -θ를 절편이라고 한다.
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%201.png)
    
    왼) 퍼셉트론 결정 함수로 최종입력 z=wTx가 이진 출력(-1 or 1)으로 압축 되는 방법
    오) 선형 분리가 가능한 두 개의 클래스 사이를 구별하는 방법
    
- 2.1.2 퍼셉트론 학습 규칙
    1. 가중치를 0 또는 랜덤한 작은 값으로 초기화 한다.
    2. 각 훈련 샘플 x(i)에서 다음 작업을 한다.
    a. 출력값 y를 계산한다.
    b. 가중치를 업데이트 한다.
    
    **수학 어쩌고 정리할것
    

## 2.2 파이썬으로 퍼셉트론 학습 알고리즘 구현

- 2.2.1 객체 지향 퍼셉트론 API
    
    Perceptron 객체를 초기화한 후 fit 메서드로 데이터에서 학습하고, 별도의 predict 매서드로 예측을 만든다. 
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%202.png)
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%203.png)
    
    퍼셉트톤 구현을 사용하여 학습률 eta와 에포크 횟수(훈련 데이터셋을 반복하는 횟수) n_iter로 새로운 Perceptron 객체를 초기화한다.
    
    ▶fit 메서드 
             -self.w_ 가중치를 백터 R로 초기화한다.     
             -m: 데이터셋에 있는 차원(특성) 개수
             -self.w_[0]: 절편   
             -이 벡터(self.w_ / 벡터 R)는 표준 편차가 0.01인 정규 분포에서 뽑은 랜덤한 작은 수를        
             담고 있음 
             -가중치를 0으로 초기화 하지 않는 이유: 학습률 파라미터 eta에 영향을 미침        
                        / 가중치가 0이라면 eta는 가중치 벡터의 방향이 아니라 크기에만 영향을 미침    
             -가중치 초기화 후 모든 개개의 샘플을 반복순회, 가중치 업데이트 
             -self.errors 리스트에 잘못 분류된 횟수 기록: 퍼셉트론을 잘 수행했는지 분석 가능
    
    ▶predict 메서드  
             -클래스 레이블에 대한 예측   
             -모델이 학습되고 난 후 새로운 데이터의 클래스 레이블을 예측할 때도 사용
    
    ▶net_input 메서드: [np.dot](http://np.dot) 함수는 벡터 점곱 wTx를 계산
    
- 2.2.2  붓꽃 데이터셋에서 퍼셉트론 훈련
    
    50개의 setosa와 50개의 versicolor 꽃에 해당하는 처음 100개의 클래스 레이블 추출
    꽃받침 길이와 꽃잎 길이 사용
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%204.png)
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%205.png)
    
    -꽃받침 길이와 꽃잎 길이 두 개의 특성 축을 따라 분포된 형태
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%206.png)
    
    -에포크 대비 잘못 분류된 오차를 그래프로 그리기
    -6번째 에포크 이후 수렴했음, 그 후 완벽하게 분류
    
    **아이패드 필기 후 첨부 그리고 이해 못함
    

## 2.3 적응형 선형 뉴런과 학습의 수렴

- 아달린
    
    -적응형 선형 뉴런
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%207.png)
    
    아달린은 연속 함수로 비용 함수를 정의, 최소화하는 핵심 개념
    아달린과 퍼셉트론의 가장 큰 차이점: 가중치를 업데이트 할 때, 단위함수가 아닌 선형 활성화 함수 사용
    -아달린: 진짜 클래스 레이블과 선형 활성화 함수의 실수 출력 값을 비교하여 모델의 오차 계산, 가증치 증가
    -퍼셉트론: 진짜 클래스 레이블과 예측 클래스 레이블을 비교
    
- 2.3.1 경사 하강법으로 비용 함수 최소화
    
    학습 과정 동안 최적화하기 위해 정의한 목적 함수이다. (최소화하려는 비용 함수가 목적 함수가될 수도 있다.) 아달린은 계산된 출력과 진짜 클래스 레이블 사이의 제곱 오차합으로 가중치를 학습하기 위한 비용 함수 J를 정의한다.
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%208.png)
    
    ![경사 하강법 알고리즘](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%209.png)
    
    경사 하강법 알고리즘
    
    : 초기 가중치를 최솟값에 도달할 때까지 이동시킴
    : 각 반복에서 경사의 반대 방향으로 진행됨
    :경사의 기울기와 학습률로 결정됨
    
- 2.3.2 파이썬으로 아달린 구현
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2010.png)
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2011.png)
    
    :퍼셉트론처럼 개별 훈련 샘플마다 평가 후 가중치를 업데이트 하지 않고, 전체 훈련 데이터셋을 기반으로 그레이디언트를 계산한다.
    :self.w_[0]: 0번째 가중치(절편)
    :self.w_[1:] 가중치 1~m까지
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2012.png)
    
    n=0.1일 때:
    비용 함수의 최소화를 찾지 못하고 오차는 에포코마다 점점 커짐. 전역 최솟값을 지나쳤기 때문.
    n=0.01일 때:
    학습률이 너무 작기 때문에 전역 최솟값에 수렴하려면 아주 많은 에포코 필요함.
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2013.png)
    
    왼) 비용이 점차 감소하여 전역 최솟값의 방향으로 이동
    오) 너무 큰 학습률은 선택하여 전역 최솟값을 지나침
    
- 2.3.3 특성 스케일을 조정하여 경사 하강법 결과 향상
    
    경사하강법은 특정 스케일을 조정하여 혜택을 볼 수 있는 많은 알고리즘 중 하나이다. 이 절에서는 표준화라고 하는 특성 스케일 방법을 사용한다. 각 특성의 평균을 0에 맞추고 특성의 표준 편차를 1(단위 분산)으로 만든다. 
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2014.png)
    
    모든 샘플에서 평균을 빼고 표준 편차로 나눈 값이다. xj는 n개의 모든 훈련 샘플에서 j번째 특성값을 포함한 벡터이다. 
    
    ![표준화가 비용 함수에 미치는 영향](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2015.png)
    
    표준화가 비용 함수에 미치는 영향
    
    2차원 분류 모델에서 모델의 가중치에 따른 비용함수의 등고선을 보여준다
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2016.png)
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2017.png)
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2018.png)
    
    학습률 n=0.01을 사용하고 표준화된 특성에서 훈련하니 아달린 모델이 수렴했다. 모든 샘플이 완벽하게 분류되더라도 SSE가 0이 되지는 않았다. 
    
- 2.3.4 대규모 머신 러닝과 확률적 경사 하강법
    
    배치 경사 하강법을 실행하면 계산 비용이 높다. 단계마다 매번 전체 훈련 데이터셋을 다시 평가 해야 하기 때문이다. 
    
    확률적 경사 하강법(반복 또는 온라인 경사 하강법)은 배치 경사 하강법의 다른 대안이다. 다음 첫 번째 수식처럼 모든 샘플에 대하여 누적된 오차의 합을 기반으로 가중치를 업데이트하는 대신 두 번째 수식처럼 각 훈련 샘플에 대해 조금씩 가중치를 업데이트한다.
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2019.png)
    
    -가중치가 더 자주 업데이트 되기 때문에 수렴 속도가 훨씬 빠르다. (하지만 오차 궤적은 배치 경사 하강법보다 훨씬 복잡하다.)
    -만족스러운 결과를 얻기 위해 훈련 샘플 순서를 무작위하게 주입하는 것이 중요하다. 
    -또 순환되지 않도록 에포크마다 훈련 데이터셋을 섞는 것이 좋다. 
    -온라인 학습으로 사용할 수 있다.(온라인 학습에서 모델은 새로운 훈련 데이터가 도착하는 대로 훈련, 많은 양의 데이터)
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2020.png)
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2021.png)
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2022.png)
    
    ▶fit 메서드
    각 훈련 샘플에 대해 가중치를 업데이트 할 것
    훈련 후 알고리즘이 수렴하는 확인하려고 에포크마다 훈련 샘플의 평균 비용을 계산함
    비용 함수를 최적화 할 때 반복적인 순환이 이러나지 않도록 매 에포크에 훈련 샘플을 섞음
    ▶partial_fit 메서드
    가중치를 다시 초기화하지 않아 온라인 학습에서 사용할 수 있음
    ▶_suffle 메서드
    0부터 100까지 중복되지 않은 랜덤한 숫자 시퀀스 생성
    이 숫자 시퀀스를 특성 행렬과 클래스 레이블 벡터를 섞는 인덱스로 사용
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2023.png)
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2024.png)
    
    ![Untitled](%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1(chap2)%208f2f13ae0d1243dbb9d1d4167a691ad0/Untitled%2025.png)
    
    평균 비용 빠르게 감소